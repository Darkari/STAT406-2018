---
title: "STAT406 - Lecture 6 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lecture slides

The lecture slides are [here](STAT406-17-lecture-6-preliminary.pdf).

## LASSO 

A different approach to perform some type of variable selection that can be
more stable than stepwise methods is to use an L1 regularization term
(instead of the L2 one used in ridge regression). Notwidthstanding the
geometric "interpretation" of the effect of using an L1 penalty, 
it can also be argued that the L1 norm is, in some cases, a convex relaxation
(envelope) of the "L0" norm (the number of non-zero elements). As a result,
estimators based on the LASSO (L1-regularized regression) will typically have some 
of their entries equal to zero. 


As the value of the penalty parameter increases the solutions change
from the usual least squares estimator (when the regularization parameter equals
to zero) to a vector of all zeroes (when the penalty constant is sufficiently 
large). This sequence of solutions is generally used 
as a way to rank (or sequence) the explanatory variables, listing them
in the order in which they "enter" (their estimated coefficient changes from 
zero to a non-zero value). 
<!-- Varying the value of the penalty term we obtain a path of solutions (much like -->
<!-- we did in ridge regression), where the vector of estimated regression -->
<!-- coefficients becomes sparser as the penalty gets stronger.  -->
We can also estimate the MSPE of each solution (on a finite
grid of values of the penalty parameter) to select one with
good prediction properties. If any of the 
estimated regression coefficients in the selected solution are exactly zero it
is commonly said that those explanatory variables are not included 
in the chosen model. 

There are two main implementation of the LASSO in `R`, one is
via the `glmnet` function (in package `glmnet`), and the other
is with the function `lars` in package `lars`. Both, of course,
compute the same estimators, but they do so in different ways. 

We first compute the path of LASSO solutions for the `credit` data
used in previous lectures:
```{r creditlasso, warning=FALSE, message=FALSE}
x <- read.table('../Lecture5/Credit.csv', sep=',', header=TRUE, row.names=1)
# use non-factor variables
x <- x[, c(1:6, 11)]
y <- as.vector(x$Balance)
xm <- as.matrix(x[, -7])
library(glmnet)
# alpha = 1 - LASSO
lambdas <- exp( seq(-3, 10, length=50))
a <- glmnet(x=xm, y=y, lambda=rev(lambdas),
            family='gaussian', alpha=1, intercept=TRUE)
```

The `plot` method can be used to show the path of solutions, just as
we did for ridge regression:

```{r creditlasso3, fig.width=5, fig.height=5}
plot(a, xvar='lambda', label=TRUE, lwd=6, cex.axis=1.5, cex.lab=1.2)
```

Using `lars::lars()` we obtain:

```{r creditlars1, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
library(lars)
b <- lars(x=xm, y=y, type='lasso', intercept=TRUE)
plot(b, lwd=4)
```

With `lars` the returned object is a matrix of regression estimators, one
for each value of the penalty constant where a new coefficient "enters" the
model:

```{r creditlars2}
# see the variables
coef(b)
b
```

The presentation below exploits the fact that the LASSO regression estimators
are piecewise linear between values of the regularization parameter where
a variable enters or drops the model.

In order to select one LASSO estimator (among the infinitely many that
are possible) we can use K-fold CV to estimate the MSPE of a few of them 
(for a grid of values of the penalty parameter, for example), and 
choose the one with smallest estimated MSPE:

```{r creditlars3, fig.width=5, fig.height=5}
# select one solution
set.seed(123)
tmp.la <- cv.lars(x=xm, y=y, intercept=TRUE, type='lasso', K=5,
                  index=seq(0, 1, length=20))
```

Given their random nature, it is always a good idea to run K-fold CV experiments 
more than once:

```{r creditlars4, fig.width=5, fig.height=5}
set.seed(23)
tmp.la <- cv.lars(x=xm, y=y, intercept=TRUE, type='lasso', K=5,
                  index=seq(0, 1, length=20))
```

We now repeat the same steps as above but using the implementation
in `glmnet`:

```{r creditcv, fig.width=5, fig.height=5}
# run 5-fold CV with glmnet()
set.seed(123)
tmp <- cv.glmnet(x=xm, y=y, lambda=lambdas, nfolds=5, alpha=1, 
                 family='gaussian', intercept=TRUE)
plot(tmp, lwd=6, cex.axis=1.5, cex.lab=1.2)
```

We ran CV again:

```{r creditcv2, fig.width=5, fig.height=5}
set.seed(23)
tmp <- cv.glmnet(x=xm, y=y, lambda=lambdas, nfolds=5, alpha=1, 
                 family='gaussian', intercept=TRUE)
plot(tmp, lwd=6, cex.axis=1.5, cex.lab=1.2)
```

Zoom in the CV plot to check the 1-SE rule:

```{r creditcv4, fig.width=5, fig.height=5}
plot(tmp, lwd=6, cex.axis=1.5, cex.lab=1.2, ylim=c(22000, 33000))
```

The returned object includes the "optimal" value of the 
penalization parameter, which can be used to 
find the corresponding estimates for the regression
coefficients:
```{r creditcv3}
# optimal lambda
tmp$lambda.min
# coefficients for the optimal lambda
coef(tmp, s=tmp$lambda.min)
# coefficients for other values of lambda
coef(tmp, s=exp(4))
coef(tmp, s=exp(4.5)) # note no. of zeroes...
```

## Compare MSPEs of Ridge & LASSO on the credit data

We now compare the MSPEs of the different 
estimators / predictors:

```{r mspecredit, warning=FALSE, message=FALSE, fig.width=5, fig.height=5, tidy=TRUE}
library(MASS)
n <- nrow(xm)
k <- 5
ii <- (1:n) %% k + 1
set.seed(123)
N <- 50
mspe.la <- mspe.st <- mspe.ri <- mspe.f <- rep(0, N)
for(i in 1:N) {
  ii <- sample(ii)
  pr.la <- pr.f <- pr.ri <- pr.st <- rep(0, n)
  for(j in 1:k) {
    tmp.ri <- cv.glmnet(x=xm[ii != j, ], y=y[ii != j], lambda=lambdas, 
                        nfolds=5, alpha=0, family='gaussian') 
    tmp.la <- cv.glmnet(x=xm[ii != j, ], y=y[ii != j], lambda=lambdas, 
                        nfolds=5, alpha=1, family='gaussian')
    null <- lm(Balance ~ 1, data=x[ii != j, ])
    full <- lm(Balance ~ ., data=x[ii != j, ])
    tmp.st <- stepAIC(null, scope=list(lower=null, upper=full), trace=0)
    pr.ri[ ii == j ] <- predict(tmp.ri, s='lambda.min', newx=xm[ii==j,])
    pr.la[ ii == j ] <- predict(tmp.la, s='lambda.min', newx=xm[ii==j,])
    pr.st[ ii == j ] <- predict(tmp.st, newdata=x[ii==j,])
    pr.f[ ii == j ] <- predict(full, newdata=x[ii==j,])
  }
  mspe.ri[i] <- mean( (x$Balance - pr.ri)^2 )
  mspe.la[i] <- mean( (x$Balance - pr.la)^2 )
  mspe.st[i] <- mean( (x$Balance - pr.st)^2 )
  mspe.f[i] <- mean( (x$Balance - pr.f)^2 )
}
boxplot(mspe.la, mspe.ri, mspe.st, mspe.f, names=c('LASSO','Ridge', 'Stepwise', 'Full'), col=c('steelblue', 'gray80', 'tomato', 'springgreen'), cex.axis=1, cex.lab=1, cex.main=2)
mtext(expression(hat(MSPE)), side=2, line=2.5)
```

## Compare MSPEs of Ridge & LASSO on the air pollution data

