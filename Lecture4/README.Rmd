---
title: "STAT406 - Lecture 4 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lecture slides

The lecture slides are [here](STAT406-17-lecture-4-preliminary.pdf).

## Estimating MSPE with CV when the model was built using the data

We learned last week that one needs to be careful when using cross-validation (in any of its flavours--leave one out, K-fold, etc.) In particular, one needs to repeat (in every fold) **everything** that was done with the training set (selecting variables, looking at pairwise correlations, AIC values, etc.)

## Correlated covariates

Technological advances in recent decades have resulted in data
being collected in a fundamentally different way from the way
it was when "classical" statistical methods were proposed.
Specifically, it is not at all uncommon to have data sets with
an abundance of potentially useful explanatory variables.
Sometimes the investigators are not sure which of them can be
expected to be useful or meaningful. In many applications one
finds data with many more variables than cases.

A consequence of this "wide net" data collection strategy is
that many of the explanatory variables may be correlated with
each other. In what follows we will illustrate some of the
problems that this can cause both when training and interpreting
models, and also with the resulting predictions.

### Significant variables "dissappear"

Consider the air pollution data set, and the fit to the
**reduced** linear regression model used previously in class:
```{r signif}
# Correlated covariates
x <- read.table('../Lecture1/rutgers-lib-30861_CSV-1.csv', header=TRUE, sep=',')
reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data=x)
round( summary(reduced)$coef, 3)
```
Note that all coefficients seem to be significant based on
the individual tests of hypothesis (with `POOR` and
`HOUS` maybe only marginally so). In this sense all 5
explanatory varibles in this model appear to be relevant.

Now, we fit the **full** model, that is, we include
all available explanatory variables in the data set:
```{r signif2}
full <- lm(MORT ~ ., data=x)
round( summary(full)$coef, 3)
```
Now we have many more parameters to estimate, and while two of
them appear to be significantly different from zero (`NONW`
and `PREC`), all the others seem to be redundant.
In particular, note that the p-values for the individual
test of hypotheses for 4 out of the 5
regression coefficients for the variables of the **reduced**
model have now become not significant.
```{r signif3}
round( summary(full)$coef[ names(coef(reduced)), ], 3)
```

### Why does this happen?

Recall that the covariance matrix of the least squares estimator involves the
inverse of (X'X), where X' denotes the transpose of the n x p matrix X (that
contains each vector of explanatory variables as a row). It is easy to see
that if two columns of X are linearly dependent, then X'X will be rank deficient.
When two columns of X are "close" to being linearly dependent (e.g. their
linear corrleation is high), then the matrix X'X will be ill-conditioned, and
its inverse will have very large entries. This means that the estimated
standard errors of the least squares estimator will be unduly large, resulting
in non-significant test of hypotheses for each parameter separately, even if
the global test for all of them simultaneously is highly significant.

### Why is this a problem if we are interested in prediction?

Although in many applications one is interested in interpreting the parameters
of the model, even if one is only trying to fit / train a model to do
predictions, highly variable parameter estimators will typically result in
a noticeable loss of prediction accuracy. This can be easily seen from the
bias / variance factorization of the mean squared prediction error (MSPE)
mentioned in class. Hence, better predictions can be obtained if one
uses less-variable parameter estimators.

### What can we do?

A commonly used strategy is to remove some explanatory variables from the
model, leaving only non-redundant covariates. However, this is easier said than
done. You have seen some strategies in other courses (stepwise variable selection, etc.)
In coming weeks we will investigate other methods to deal with this problem.







## "Direct" comparison of models -- AIC

A different approach to comparing competing models is to assess how "close" they 
are from the actual stochastic process generating the data (here "stochastic process"
refers to the random mechanism that generated the data). In order to do this one
needs:

a. a distance / metric (or at least a "quasimetric") between models; and 
a. a way of estimating this distance when the "true" model is unknown.

AIC provides an unbiased estimator of the Kullback-Leibler divergence 
between the estimated model and the "true" one. 


## Shrinkage methods / Ridge regression 

To manage correlated explanatory variables...

### Selecting the amount of shrinkage




