---
title: "STAT406 - Lecture 10 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Lecture slides

The lecture slides are [here](STAT406-17-lecture-10-preliminary.pdf).

## Regression trees 

Trees provide a non-parametric regression estimator that is
able to overcome a serious limitation of "classical non-parametric"
estimators (like those based on splines, or kernels) 
when several (more than 2 or 3) explanatory variables are
available. 

Below we first describe the problem afflicting classical 
non-parametric methods (this is also known as the "curse of dimensionality")
and then describe how to compute regression trees in `R` using the
`rpart` package (although other implementations exist). 
Details were discussed in class. 

### Curse of dimensionality

Suppose  you have a random sample of *n = 100* observations,
uniformly distributed on the [0, 1] interval. How many do you 
expect to find within 0.25 of the middle point of the
interval (i.e. how many will be between 0.25 and 0.75)?
A trivial calculation shows that the expected number of
observations falling between 0.25 and 0.75 will be *n/2*,
in this case *50*. This is easy verified with a simple
numerical experiment:
```{r curse, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
# X ~ U(0,1)
# how many points do you expect within 0.25 of 1/2?
set.seed(1234)
n <- 100
x <- runif(n)
( sum( abs(x-1/2) < 0.25 ) )# half the width of the dist'n
```
(wow! what are the chances?)

Consider now a sample of 100 observations, each with 
5 variables (5-dimensional observations), 
uniformly distributed in the 5-dimensional unit cube 
(*[0,1]^5*). How many do you expect to see in the
*central hypercube* with sides [0.25, 0.75] x [0.25, 0.75] ... 
x [0.25, 0.75] = [0.25, 0.75]^5? A simple experiment shows
that this number is probably rather small:
```{r curse.p5, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
p <- 5
x <- matrix( runif(n*p), n, p)
# how many points in the hypercube (0.25, 0.75)^p ?
tmp <- apply(x, 1, function(a) all(abs(a-1/2)<0.25))
( sum(tmp) )
```
In fact, the expected number of observations 
in that central hypercube is exactly *n / 2^5*, 
which is approximately *3* when *n = 100*. 

A relevant question for our local regression estimation
problem is: "how large should our sample be if we want
to have about 50 observations in our central hypercube?". 
Easy calculations show that this number is *50 / (1/2)^p*,
which, for *p = 5* is *1600*. Again, we can verify
this with a simple experiment:
```{r curse.2, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
# how many obs do we need to have 50 in the hypercube?
n <- 50 / (0.5^p)
x <- matrix( runif(n*p), n, p)
# how many points in the hypercube (0.25, 0.75)^p ?
tmp <- apply(x, 1, function(a) all(abs(a-1/2)<0.25))
( sum(tmp) )
```
However, when *p = 10*, in order to expect
50 observations in the central [0.25, 0.75] hypercube
we need a sample of size *n = 51,200* and when 
*p = 20* we need over 52 million observations
to have (just!) 50 in the central hypercube!

Another way to illustrate this problem is to 
ask: "given a sample size of *n = 1000*, say, how wide / large
should the central hypercube be to expect 
about *50* observations in it?". The answer is
easily found to be *1 / (2 (n/50)^(1/p))*, which for 
*n = 1000* and *p = 5* equals 0.27, with 
*p = 10* is 0.37 and with *p = 20* is 
0.43, almost the full unit hypercube!

In other words, in moderate to high dimensions,
*local neighbourhoods* are either empty
or not *local*.
```{r curse.3, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
# how wide should the hypercube be to get 50 neighbours
# with sample of 1000 points?
n <- 1000
p <- 20
( h <- 1 / ((n / 50)^(1/p) * 2) )
# the sides of the "central hypercube" should be:
( c(0.50 - h, 0.50 + h) )
# verify it with a single sample:
x <- matrix( runif(n*p), n, p)
# how many points in the hypercube (0.25, 0.75)^p ?
tmp <- apply(x, 1, h=h, function(a,h) all(abs(a-1/2)<h))
( sum(tmp) )
```

### Regression trees as constrained non-parametric regression

Regression trees provide an alternative non-regression
estimator that works well, even with many available features. 


```{r tree, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
library(rpart)
# Regression trees
data(Boston, package='MASS')

# split data into a training and
# a test set
set.seed(123456) 
n <- nrow(Boston)
ii <- sample(n, floor(n/4))
dat.te <- Boston[ ii, ]
dat.tr <- Boston[ -ii, ]

set.seed(123)
bos.t <- rpart(medv ~ ., data=dat.tr, method='anova')
plot(bos.t, uniform=TRUE) 
text(bos.t, pretty=TRUE)

plot(bos.t, uniform=FALSE)
text(bos.t, pretty=TRUE)


# predictions on the test set
pr.t <- predict(bos.t, newdata=dat.te, type='vector')
with(dat.te, mean( (medv - pr.t)^2) )


# full linear model
bos.lm <- lm(medv ~ ., data=dat.tr)
pr.lm <- predict(bos.lm, newdata=dat.te)
with(dat.te, mean( (medv - pr.lm)^2) )

# use stepwise try to make it better
library(MASS)
null <- lm(medv ~ 1, data=dat.tr)
full <- lm(medv ~ ., data=dat.tr)
bos.aic <- stepAIC(null, scope=list(lower=null, upper=full), trace=FALSE)
pr.aic <- predict(bos.aic, newdata=dat.te)
with(dat.te, mean( (medv - pr.aic)^2 ) )

# LASSO?
library(glmnet)
x.tr <- as.matrix(dat.tr[,-14])
y.tr <- as.vector(dat.tr$medv)
set.seed(123)
bos.la <- cv.glmnet(x=x.tr, y=y.tr, alpha=1)
x.te <- as.matrix(dat.te[,-14])
pr.la <- predict(bos.la, s='lambda.1se', newx=x.te)
with(dat.te, mean( (medv - pr.la)^2 ) )
```

#### Pruning regression trees

```{r prune, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
# build a big tree (overfitting?)
myc <- rpart.control(minsplit=3, cp=1e-8, xval=10)
set.seed(123)
bos.to <- rpart(medv ~ ., data=dat.tr, method='anova',
                control=myc)
plot(bos.to, compress=TRUE) # type='proportional')
# predictions are poor, unsurprisingly
pr.to <- predict(bos.to, newdata=dat.te, type='vector')
with(dat.te, mean((medv - pr.to)^2) )

# prune it


# find cp corresponding to smallest cv-estimated
# prediction error

plot(bos.to)
printcp(bos.to)
b <- bos.to$cptable[which.min(bos.to$cptable[,"xerror"]),"CP"]
bos.t3 <- prune(bos.to, cp=b)
plot(bos.t3, uniform=FALSE)
text(bos.t3, pretty=TRUE)

# predictions are better
pr.t3 <- predict(bos.t3, newdata=dat.te, type='vector')
with(dat.te, mean((medv - pr.t3)^2) )

# what if we prune the original tree?
b <- bos.t$cptable[which.min(bos.t$cptable[,"xerror"]),"CP"]
bos.t4 <- prune(bos.t, cp=b)
plot(bos.t4)
plot(bos.t4, uniform=FALSE)
text(bos.t4, pretty=TRUE)
# same as original
plot(bos.t, uniform=FALSE)
text(bos.t, pretty=TRUE)


pr.t4 <- predict(bos.t4, newdata=dat.te, type='vector')
with(dat.te, mean((medv - pr.t4)^2) )
# same tree, really
with(dat.te, mean((medv - pr.t)^2) )
```

