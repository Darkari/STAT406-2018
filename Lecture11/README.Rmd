---
title: "STAT406 - Lecture 11 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### LICENSE
These notes are released under the 
"Creative Commons Attribution-ShareAlike 4.0 International" license. 
See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)
and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). 

## Lecture slides

The lecture slides are [here](STAT406-17-lecture-11-preliminary.pdf).

## Instability of regression trees 

Trees can be rather unstable, in the sense that small changes in the
training data set may result in relatively large differences in the
fitted trees. As a simple illustration we randomly split the 
`Boston` data used before into two halves and fit a regression
tree to each portion. We then display both trees.

```{r inst1, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
# Instability of trees...
library(rpart)
data(Boston, package='MASS')
set.seed(654321)
n <- nrow(Boston)
ii <- sample(n, floor(n/2))
dat.t1 <- Boston[ -ii, ]
bos.t1 <- rpart(medv ~ ., data=dat.t1, method='anova')
plot(bos.t1, uniform=FALSE, margin=0.05)
text(bos.t1, pretty=TRUE, cex=.8)
```

```{r inst2, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
dat.t2 <- Boston[ ii, ]
bos.t2 <- rpart(medv ~ ., data=dat.t2, method='anova')
plot(bos.t2, uniform=FALSE, margin=0.05)
text(bos.t2, pretty=TRUE, cex=.8)
```

Although we would expect both random halves of the same (moderately large) 
training set to beat least qualitatively similar, 
Note that the two trees are rather different. 
To compare with a more stable predictor, we fit a linear
regression model to each half, and look at the two sets of estimated
coefficients side by side:
```{r inst3, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
# bos.lmf <- lm(medv ~ ., data=Boston)
bos.lm1 <- lm(medv ~ ., data=dat.t1)
bos.lm2 <- lm(medv ~ ., data=dat.t2)
cbind(round(coef(bos.lm1),2),
round(coef(bos.lm2),2))
```
Note that most of the estimated regression coefficients are 
similar, and all of them are at least qualitatively comparable. 

## Bagging

One strategy to obtain more stable predictors is called
**Bootstrap AGGregatING** (bagging). It can be applied to
many predictors (not only trees), and it generally results
in larger improvements in prediction quality when it is used with predictors
that are flexible (low bias), but highly variable.

The justification and motivation were discussed in class. Intuitively
we are averaging the predictions obtained from an estimate of the 
"average prediction" we would have computed had we had access to 
several (many?) independent training sets (samples). 

There are several (many?) `R` packages implementing
bagging for different predictors, with varying degrees of 
flexibility (the implementations) and user-friendliness. 
However, for pedagogical and illustrative purposes, in these notes I will
*bagg* by hand.

### Bagging by hand

Again, to simplify the discussion and presentation, in order to evaluate 
prediction quality I will split the 
data (`Boston`) into a training and a test set. We do this now:
```{r bag1, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
set.seed(123456)
n <- nrow(Boston)
ii <- sample(n, floor(n/4))
dat.te <- Boston[ ii, ]
dat.tr <- Boston[ -ii, ]
```
I will now train $N = 5$ trees and average their predictions. 
Note that, in order to illustrate the process more
clearly, I will compute and store the $N \times n_e$
predictions, where $n_e$ denotes the number of observations in 
the test set. This is not the best (most efficient) way to implement bagging,
but the purpose here is to understand what we are doing. 

First create an array where we will store all the predictions:
```{r bag2, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
N <- 5
myps <- array(NA, dim=c(nrow(dat.te), N))
con <- rpart.control(minsplit=3, cp=1e-3, xval=1)
```
The last object (`con`) contains my options to train large
(potentially overfitting) trees. 
```{r bag3, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
n.tr <- nrow(dat.tr)
set.seed(123456)
for(j in 1:N) {
  ii <- sample(n.tr, replace=TRUE)
  tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con)
  myps[,j] <- predict(tmp, newdata=dat.te, type='vector')
}
pr.bagg <- rowMeans(myps)
with(dat.te, mean( (medv - pr.bagg)^2 ) )
```
And compare with predictions from the pruned tree, and the
ones from other predictors discussed in the previous note:
```{r bag4, fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
myc <- rpart.control(minsplit=3, cp=1e-8, xval=10)
set.seed(123)
bos.to <- rpart(medv ~ ., data=dat.tr, method='anova',
                control=myc)
b <- bos.to$cptable[which.min(bos.to$cptable[,"xerror"]),"CP"]
bos.t3 <- prune(bos.to, cp=b)
pr.t3 <- predict(bos.t3, newdata=dat.te, type='vector')
with(dat.te, mean((medv - pr.t3)^2) )
```

What if we *bagg* $N = 10$ trees? 
```{r bag10, fig.width=5, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE}
N <- 10
myps <- array(NA, dim=c(nrow(dat.te), N))
n.tr <- nrow(dat.tr)
set.seed(123456)
for(j in 1:N) {
  ii <- sample(n.tr, replace=TRUE)
  tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con)
  myps[,j] <- predict(tmp, newdata=dat.te, type='vector')
}
pr.bagg <- rowMeans(myps)
with(dat.te, mean( (medv - pr.bagg)^2 ) )
```
or $N = 100$ trees? 
```{r bag100, fig.width=5, fig.height=5, message=FALSE, warning=FALSE, echo=FALSE}
N <- 100
myps <- array(NA, dim=c(nrow(dat.te), N))
n.tr <- nrow(dat.tr)
set.seed(123456)
for(j in 1:N) {
  ii <- sample(n.tr, replace=TRUE)
  tmp <- rpart(medv ~ ., data=dat.tr[ii, ], method='anova', control=con)
  myps[,j] <- predict(tmp, newdata=dat.te, type='vector')
}
pr.bagg <- rowMeans(myps)
with(dat.te, mean( (medv - pr.bagg)^2 ) )
```

Should we consider higher values of $N$? How about other
training / test splits? Should we use CV instead? 





### Bagging a regression spline

<!-- data(lidar, package='SemiPar') -->
<!-- plot(logratio~range, data=lidar, pch=19, col='gray', cex=2.5) -->

<!-- # split in training and testing  -->
<!-- set.seed(123456)  -->
<!-- n <- nrow(lidar) -->
<!-- ii <- sample(n, floor(n/5)) -->
<!-- lid.te <- lidar[ ii, ] -->
<!-- lid.tr <- lidar[ -ii, ] -->

<!-- bound <- c(min(lidar$range), max(lidar$range)) -->

<!-- library(splines) -->

<!-- a <- lm(logratio ~ bs(x=range, df=30, Boundary.knots=bound), data=lid.tr) -->
<!-- oo <- order(lid.tr$range) -->
<!-- plot(logratio~range, data=lid.tr, pch=19, col='gray', cex=2.5) -->
<!-- lines(predict(a)[oo] ~ lid.tr$range[oo], lwd=4, col='red') -->
<!-- pr.of <- predict(a, newdata=lid.te) -->
<!-- mean( (lid.te$logratio - pr.of)^2 ) -->

<!-- N <- 100 # 5 500 1500 -->
<!-- xseq <- seq(min(lidar$range), max(lidar$range), length=500) -->
<!-- myps <- matrix(NA, nrow(lid.te), N) -->
<!-- myse <- matrix(NA, length(xseq), N) -->
<!-- set.seed(123456) -->
<!-- n.tr <- nrow(lid.tr) -->
<!-- for(i in 1:N) { -->
<!--   ii <- sample(n.tr, replace=TRUE) -->
<!--   a.b <- lm(logratio ~ bs(x=range, df=30, Boundary.knots=bound), data=lid.tr[ii,]) -->
<!--     # try(lm(logratio ~ bs(x=range, df=30, Boundary.knots=bound), data=lid.tr[ii,]), silent=TRUE) -->
<!-- #  if(class(a.b) != 'try-error') { -->
<!--   myps[,i] <- predict(a.b, newdata=lid.te) -->
<!--   myse[,i] <- predict(a.b, newdata=list(range=xseq)) -->
<!-- #  } -->
<!-- } -->
<!-- pr.ba <- rowMeans(myps)# , na.rm=TRUE) -->
<!-- mean( (lid.te$logratio - pr.ba)^2 ) -->
<!-- pr.se <- rowMeans(myse)# , na.rm=TRUE) -->


<!-- pr.ofse <- predict(a, newdata=list(range=xseq)) -->
<!-- plot(logratio~range, data=lidar, pch=19, col='gray', cex=2.5) -->
<!-- points(logratio~range, data=lid.tr, pch=19, col='gray30', cex=.75) -->
<!-- points(logratio~range, data=lid.te, pch=19, col='blue', cex=.75) -->
<!-- lines(pr.ofse ~ xseq, lwd=4, col='red') -->
<!-- lines(pr.se ~ xseq, lwd=4, col='magenta') -->



